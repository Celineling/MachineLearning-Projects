{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My study note for pre-processing the text_based data for Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we have text_based data including names and descriptions of the products, and before we build and fit a classifier, we need to deal with the text_based data.<br>\n",
    "\n",
    "> The following is the methods and codings for preprecessing such data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Filter data and Stem Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It has been decided that products with names or description other than in English will not be considered. The \"langdetect\" package does this work. Furthermore, it is reasonable to expect that the names and description not to be too short, like more than 5 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "def filterWrapper(textAndCate):\n",
    "    try:\n",
    "        if detect(textAndCate[0])=='en' and len(textAndCate[0])>5:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except LangDetectException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It makes sense to stem the text. That means we will treat words in its different forms as one, for instance, “shoe” and “shoes”. We can utilize the \"nltk (natural language toolkit)\" package to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stemAndConCat(text):\n",
    "    stemmedText = ''\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            stemmedText = stemmedText + stemmer.stem(word)+''\n",
    "    return stemmedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we now have a list comprised of stemmed text. We also have the corresponding parent categories (their indexes), since we would like to begin with the classifier on the higher level, i.e. classifying products into parent categories. The list is shuffled before being split.<br>\n",
    "\n",
    "> Next, we transform the text into vectors using the \"TF-IDF algorithm\", before feeding it into the model. This is accomplished by using the \"pipeline\", which handles a series of transformation. To speed up the training process later, we can cache the transformation. <br>\n",
    "\n",
    "> All of the algorithms are implemented in the \"sklearn\" package, and we can simply import and call the APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling\n",
    "#Suppose My_Data is the name of my sampling data set, \n",
    "#in order to run code successfully, Here randomly define a data frame\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "My_Data = pd.DataFrame(data=[['A',\"B\",\"C\",\"D\",\"E\",\"Text\"] for i in range(1,6)])\n",
    "\n",
    "data_target_list = []\n",
    "for cate_tuple in My_Data.itertuples(): \n",
    "    data_target_list.append(((cate_tuple[1]+''+cate_tuple[2]).lower().replace('description',' '), cate_tuple[4]))#4 is parent categ.\n",
    "\n",
    "data_target_list = list(filter(filterWrapper, data_target_list))\n",
    "sizeOfList = len(data_target_list)\n",
    "shuffle(data_target_list)\n",
    "data_list = [stemAndConcat(item[0]) for item in data_target_list]\n",
    "target_list = [item[1] for item in data_target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelining Transformation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "def getPipeline(clf):\n",
    "    count_vect = CountVectorizer(ngram_range = (1,2), stop_words = 'english', analyzer = 'word')\n",
    "    tfidfTransformer = TfidTransformer()\n",
    "    return Pipeline([('count', count_vect), ('tf', tfidTransformer), ('clf', clf)], memory=cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, we finish preprocessing the data. Then, we could use Classification methods, like K-means, Hierarchical classification algorithms to train a classifiers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
